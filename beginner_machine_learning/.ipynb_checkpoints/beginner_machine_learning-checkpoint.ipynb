{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A basic machine learning pipeline includes:\n",
    "1. Understand the problem\n",
    "2. Explore the data\n",
    "3. Pre-process (clean) the data\n",
    "4. Divide the data into training set and testing set\n",
    "5. Train a machine learning classifier\n",
    "6. Test the performance of the trained model\n",
    "\n",
    "In this practical, we will go through a toy example to get an intuition of these key-points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understand the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step includes identifying the problem. Is it a regression or a classification problem? If it is a classification problem, do you want to perform a supervised or a unsupervised classification? Do you already know the true classes of the given data or do you rather want to find the intrinsic clusters in the data and then draw an inference? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv ('./data/data.csv')\n",
    "data.shape\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this a classification problem? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of the columns contain any missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see anything odd in this data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize this data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vizualizing the data, whenever possible, not only helps to identify outliers/anomalies in the data but also helps to get an intuition of the underlying decision boundary best separating the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.scatterplot(x=\"Feature1\", y=\"Feature2\", hue=\"Class\", data=data, \n",
    "                edgecolor='black', markers= [\"s\", \"^\"], style= \"Class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which decision boundary would best separate the two classes?\n",
    "\n",
    "How does adding more noise affect the ease of finding a decision boundary?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pre-process the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are there any outliers in the plot above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Divide the data into training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will use 75% of the data for training the classifier and remaining 25% of the data for testing the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Define a random state for reproducibility of reuslts\n",
    "randomState = 123\n",
    "# Play around with the split size to see how increasing or decreasing \n",
    "# the training data size affect the classifier performance in the next steps\n",
    "testSize = 0.25\n",
    "dataTrain, dataTest = train_test_split(data,test_size=testSize, random_state = randomState)\n",
    "\n",
    "# Check that the splitting worked perfectly\n",
    "print(dataTrain.shape, dataTest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yTrain = dataTrain.pop('Class')\n",
    "xTrain = dataTrain\n",
    "\n",
    "yTest = dataTest.pop('Class')\n",
    "xTest = dataTest\n",
    "\n",
    "print(xTrain.shape, yTrain.shape, xTest.shape, yTest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train a machine learning classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only use the training set for this analysis!! To test the performance of the classifier on the real-world data in a fair way, it is better to keep the testing set as strictly unseen by the classifier. This can be done by excluding it completely while training the classifier. This means that the classfier learns to predict the classes using the training data and we then check how well it performs on the new or unseen data using the testing data.\n",
    "\n",
    "To evaluate the performance of the classifier, we will compute the $accuracy$ of predictions, defined as:\n",
    "\n",
    "$$Accuracy = \\frac{Number\\, of\\, data\\, points\\, correctly\\, predicted}{Total\\, number\\, of\\, data\\, points}$$\n",
    "\n",
    "Several other metrics exist to evaluate the performance of the classifier but is out of the scope of this practical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Initializing classifiers\n",
    "clf1 = LogisticRegression(random_state=randomState, solver='lbfgs')\n",
    "clf2 = RandomForestClassifier(random_state=randomState, n_estimators=100)\n",
    "clf3 = GaussianNB()\n",
    "clf4 = SVC(gamma='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "import itertools\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "import numpy as np\n",
    "\n",
    "# Training 4 different classifiers to analyze their performance on the training data\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "labels = ['Logistic Regression', 'Random Forest', 'Naive Bayes', 'SVM']\n",
    "for clf, lab, grd in zip([clf1, clf2, clf3, clf4],\n",
    "                         labels,\n",
    "                         itertools.product([0, 1], repeat=2)):\n",
    "    clf.fit(xTrain, yTrain)\n",
    "    \n",
    "    #Plotting the decision boundaries for the trained classifiers\n",
    "    ax = plt.subplot(gs[grd[0], grd[1]])\n",
    "    fig = plot_decision_regions(X=xTrain.values, y=yTrain.values.astype(np.integer), clf=clf, legend=2)\n",
    "    #print the classification score in the title\n",
    "    score = clf.score(xTrain,yTrain)\n",
    "    plt.title('%s, Accuracy = %1.3f' %(lab,score))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which classifier gives the best decision boundary?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test the performance of the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only use the testing set for this analysis!! Because your classifier was trained on the training set, using the same data set again for testing its performance will give overly-optimistic results and will fail to serve the purpose of doing a \"test\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestClassifier = # Enter the best classifier found above to see its performance on the testing data\n",
    "yPredict = bestClassifier.predict(xTest)\n",
    "ax = plt.subplot()\n",
    "fig = plot_decision_regions(X=xTest.values, y=yTest.values.astype(np.integer), clf=bestClassifier, legend=2)\n",
    "plt.title(\"Best classifier\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the \"best classifier\" according to our training data performs on the new data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the metrics\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(yTest,yPredict)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the problem at hand, it might make sense to look into other performance metrics as well, other than just using the accuracy as in this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this has been exciting so far, it is worth checking out the following concepts (in no order of importance, i.e. they are all important to know!!) for streamlining your pipeline according to the given data and the problem at hand.\n",
    "1. Performance metrics\n",
    "2. Bias vs Variance\n",
    "3. Cross-validation\n",
    "4. Available classifiers\n",
    "5. Hyperparameter tuning\n",
    "6. Dimentionality reduction\n",
    "7. Regularization\n",
    "8. Categorical vs continuous data\n",
    "9. Loss functions\n",
    "10. Data carpentry"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
