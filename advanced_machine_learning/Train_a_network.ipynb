{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from dataset import NucleiDataset\n",
    "from utils import *\n",
    "from losses import focal_loss, dice_loss\n",
    "from unet import UNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to set the path of the data and wrap the data in a so-called dataloader, that will be supplying data samples to the network during the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = '/g/kreshuk/zinchenk/courses/EMBL_BTM_2019/advanced_machine_learning/nuclei_train_data'\n",
    "train_data = NucleiDataset(TRAIN_DATA_PATH, RandomCrop(256))\n",
    "train_dataloader = DataLoader(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the data looks like. We have raw input images and ground truth - manually annotated nuclei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_dataset(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to have some data not used for training to check how the model performs on data it hasn't seen before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATA_PATH = '/g/kreshuk/zinchenk/courses/EMBL_BTM_2019/advanced_machine_learning/nuclei_test_data'\n",
    "test_data = NucleiDataset(TEST_DATA_PATH, RandomCrop(256))\n",
    "test_dataloader = DataLoader(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_dataset(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we will load the model that we want to train. The architecture is called UNet and it has been steadily outperforming the other architectures in segmenting biological and medical images. This is how it looks like:\n",
    "\n",
    "<img src=\"./imgs/u-net.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's set the hyperparameters we would need further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LAYERS = 3    # determines the capacity and the 'depth' (filed of view) of the network\n",
    "IN_FILTERS = 32   # the number of the feature maps in the first layer - also affects the capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=UNet(IN_FILTERS, NUM_LAYERS)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, test_loader, num_epochs=10):\n",
    "    dataset_size = len(train_loader)   #how many samples we have\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-' * 10)\n",
    "        model.train()\n",
    "        train_loss = 0.0    # the value that we will backprop\n",
    "        train_accuracy = 0.0    # just a helpful (for us) metric\n",
    "        train_iou = 0.0    # another helpful metric\n",
    "        count = 0\n",
    "        for images, masks in iterate(train_loader):\n",
    "            count += 1\n",
    "            optimizer.zero_grad()    # erase all the gradient from the previous steps\n",
    "            outputs = model(images)    # predict\n",
    "            predictions = (outputs > 0.5)    # binarize the predictions to get a mask\n",
    "            if count % 10 == 0:    # every tenth iteration show how we are performing\n",
    "                show_images(images, masks, predictions)\n",
    "            loss = focal_loss(outputs, masks)    # calculate the loss between the predictions and the ground truth\n",
    "            accuracy = torch.mean((predictions == masks.byte()).float())    # how much is the prediction similar to the ground truth? pixelwise\n",
    "            iou = get_iou(predictions, masks.type(torch.bool))    # calculate the intersection over union (explained below)\n",
    "            loss.backward()    # compute the gradients for every neuron\n",
    "            optimizer.step()    # backpropagate!\n",
    "            train_loss += loss.item()\n",
    "            train_accuracy += accuracy.item()\n",
    "            train_iou += iou.item()\n",
    "        epoch_loss = train_loss / dataset_size\n",
    "        epoch_accuracy = train_accuracy / dataset_size\n",
    "        epoch_iou = train_iou / dataset_size\n",
    "        print('Training loss is {:.6f}, iou is {:.6f}, accuracy is {:.6f}'.format(epoch_loss, epoch_iou, epoch_accuracy))\n",
    "        evaluate(test_loader, model)    # every epoch we want to check how the model performs on a previously unseen data\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the model to see how the prediction accuracy changes over the training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train(model, optimizer, train_dataloader, test_dataloader, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how the IOU (intersection over union) works:\n",
    "<img src=\"./imgs/iou1.png\">\n",
    "<img src=\"./imgs/iou2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
